---

### GKE Version upgrade

GKE 버전 업그레이드  :  호출 가능한 api 변경 이슈가 있음

api 변경 이슈에 관해서 확인 후 

담당자에게

개발기/검수기의 경우 upgrade 선 적용 후  서비스 점검 일정 문의

상용기의 경우는 노드가 변경되면서 파드 재생성 → 

서비스 중단 발생하므로 일정 조율 후에 진행.

---

### AWS cloudwatch

unhealthy 확인

- Target Group의 상태 확인하기

404 에러

- 같은 경로를 갖는 2개의 TG가 있는데, 그 중 1개만 unhealthy → 경로는 정상
- port 3000 , 8080 중에 3000 port만 unhealhty → 그런데 보안 그룹을 확인해보니 80 포트와 443 포트를 열어두고 모든 ip에서 받음 . 보안 그룹에서의  port가 문제라면 8080 포트도 문제가 있어야 하는거 아닌가? … 목적지 ip가 다르기 때문에 무의미한 질문인가?

404 에러 

- api 가 잘못 설정되어 있음

### 인프라 운영자로서 해야 할 일

- TargetGroup에서 healthy code를 404로 바꿔보고, healthy로 뜨는지 확인한다
- healthy로 뜰 경우 code를 404로 반환하긴 하는 것이기 때문에 WAS 서버 자체에는 문제가 없다
- 이를 해결하기 위해서는 helthy code에 404를 추가하거나, 개발사에서 404 code를 해결해주는 방법이 있음을 알린다.
- 404는 URL오류일 때가 많음

Q.  TG에서 unhealthy가 뜬 ip를 갖는 서버에 접속하는 방법은?

---

### 정책 설정 CloudXper 연동

1. policy 생성

Q.  KMS, glue, redshift, airflow 가 무엇인가?7

1. Role 생성 및 권한 부여

---

### Bastion서버 역할?

Bastion은 VPC를 대표하는 문지기 역할을 한다.

외부와 통신이 가능하며 문지기로 쓸 놈이다.

만드는 법 : Public subnet에 퍼블릭 ip 를 받는 인스턴스를 만든 후에, 해당 VPC의 다른 인스턴스들이 그 트래픽만 허용하도록 하면 bastion이 됨

(보안에 민감한 서버들은 전부 Private에 넣고 Public에 bastion을 따로 만드는게 국룰임)

(bastion, proxy 거기서 거기임)

---

### LB, Target Group , ASG의 상호작용?

**LB** 

- 대상 그룹은 지정한 프로토콜과 포트 번호를 사용하여 트래픽을 라우팅함.
- LB는 대상 그룹에 등록된 서버들에게 기능을 함
- 

---

### 인터넷 없이 통신하기

- LAN
    - 작은 지리적 범위
- 인트라넷
    - 기업이나 조직 내부에서 사용되는 사설 네트워크
- DX
    - 기업 네트워크 - AWS 리전 간에 직접적인 네트워크 연결 구축

---

### 

---

- ECR | ECS | EKS
    - ECR을 통해서 EKS 파드들을 관리한다.
    - EC2 인스턴스가 컨테이너 리포지토리 및 이미지에 액세스 할 수 있다.
- AuroraDB | DynamoDB | RDS
    - RDS
        - 자동 백업
        - DB 스냅샷
- WAF
- Deep Security

---

### Pod Request , Limit

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0af99f2e-8fd6-4b1d-b06a-d80ab9501941/f74b9c6a-7077-4ac9-b194-7f3a2a9ece10/Untitled.png)

상황 : 노드 1, 2, 3이 존재하고 한 노드는 4core cpu이다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0af99f2e-8fd6-4b1d-b06a-d80ab9501941/3ec0dc5f-3414-4a8f-b09b-55652eec7e7f/Untitled.png)

노드 안에는 여러 파드들이 떠있다.

이때 파드는 request 값과 limit 값을 갖는데, request는 노드 안에서 pod가 독립적으로 확보하는 공간이다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0af99f2e-8fd6-4b1d-b06a-d80ab9501941/e1cc04a5-2f18-4acc-84fe-a7c73a800bb7/Untitled.png)

pod의 request는 pod 가 확보하는 공간이고, limit는 pod가 사용할 수 있는 최대치이다. 

HPA란 수평적 파드 오토스케일링으로, 파드의 request 70%를 사용할 경우 스케일 아웃이 되게끔 설정되어있다.

문제는 pod가 스케일 인이 될 때는 알림이 발생하는데 현재 상황은 이 request값이 너무 낮게 측정되어 있어서 알림이 빈번히 발생하는 상황이다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0af99f2e-8fd6-4b1d-b06a-d80ab9501941/815e23d7-0485-4bb6-8d0d-d157c541d7d2/Untitled.png)

노드 별 여유 CPU 공간이 각각 다르다. 클러스터로 묶여있기 때문에 그림과 같은 상황에서 500짜리 CPU를 쓰는 파드가 할당되려면 노드 1 또는 노드 3에 배치될 것이다.

---

### AWS SG / NACL (Feat . statefull - stateless)

AWS의 SG는 Statefull이다. 

그렇기 때문에 어떤 요청을 받는 인바운드 규칙이 허용되었다면, 그 요청에 대한 응답을 보낼 때 아웃바운드 규칙은 따지지도 않고 허용을 해서 보낸다. 반대의 경우에도 어떤 요청을 보내는 아웃바운드 규칙이 허용되었다면, 그 요청에 대한 응답을 받을 때 인바운드 규칙은 따지지 않고 허용한다

NACL은 Stateless이다.

어떤 요청/응답이건 이전 상태와 상관없이 그때마다 인바운드 규칙, 아웃바운드 규칙을 검사한다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0af99f2e-8fd6-4b1d-b06a-d80ab9501941/6534b4d5-816a-444c-8588-7a84406c0765/Untitled.png)

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0af99f2e-8fd6-4b1d-b06a-d80ab9501941/7087679c-c2bf-40c7-8a3d-7e5838713ac1/Untitled.png)

---

### CloudXper 연동

1. CloudXper 역할 생성 → ARN 생성
2. NextSPOC에서 CSR 작성 (계정 별 신청)
    1. 계정명
    2. 계정번호 제대로!
3. CSR 처리되면 CloudXper에 ARN 보내주기

---

### CMS 연동

1. 토픽생성
2. Shared VPC와 연동
3. CMS- 모니터링설정 - VM - Log 
    1. 파일절대경로 복사
    2. 신규등록에 파일 절대경로 붙이기
    3. Host 선택 - COMMON - Shaerd 상용기 계정
    4. 심각도 CRITICAL, 통보담당-SVR
4. 알람발생시 토픽을 제대로 설정하기!!

---

### User 생성 후 Sudo 권한 부여 + pw 설정

- useradd ~
- usermod —append -G wheel ~
- passwd ~

---

### 스케쥴러 설정

vm → 인스턴스 일정 → cron 형식으로 설정가능

노드 / DB 는 cloud scheduler에서 따로 설정 

DB는 먼저 켜고, 늦게 끈다

DB : Always , never 확인

노드 : setsize 1, 0 맞는지 확인

→ 모니터링에서 VM cpu 사용량 확인으로 잘 되어있는지 확인하기

→ cloud scheduler에서 내가 설정했던 스케쥴 대상

---

### EKS 버전 업그레이드

GKE와 다르게 EKS는 AMI 커스터마이징이 가능함

AWS의 개발/검수기에서 쓰인 AMI 버전과 동일하게, 상용기 작업 전에

시작 템플릿이 미리 만들어놓기!

---

### Q. AWS - ASG →모니터링 → 인스턴스 에서 보이는 메트릭 들은 ASG가 관리하는 인스턴스들의 평균 값을 나타내는가? 인스턴스가 2개 이상인데, 왜 모니터링의 그래프가 1개만 보이는지?

- 평균값이다.
- 두개의 인스턴스를 각각 확인해보면, 그 평균값이 ASG-모니터링-인스턴스에서 보이는 값과 똑같다.

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0af99f2e-8fd6-4b1d-b06a-d80ab9501941/def9ebe4-70e0-4a65-96b7-108238f28f6e/Untitled.png)

주황색 - 인스턴스 1

초록색 - 인스턴스 2

파란색 - 평균

---

### SSH

공개키를 갖고있는 곳에 접근을 위해 자신의 비밀키를 건내고 해독이 가능하면 연결이 가능한 방식.

EX : git과의 연동

4848local에서 ssh-keygen을 통해 키를 생성하고 

공개키를 git에 올리고 연동

---

### 인증서 체인

인증서 - 중간인증서 - Root 인증서

---

### *** AutoScale , Launch Template , AMI — EKS

AWS의 오토스케일 기능을 사용할 때 런치템플릿을 기준으로 EC2를 생성한다

런치템플릿은 AMI에 베이스를 두는 템플릿이다

ASG의 LT를 변경하면 EC2가 변경하는 시점에 최신 버전으로 자동 재생성 될 줄 알았지만 그렇지 않다.

LT 변경 후에 스케일 인이 되는 상황이 발생하면 그 LT를 베이스로 한 EC2가 생성이 되지만 스케일 인이 되는 상황이 발생하지 않는다면 자동 재생성 되지는 않는다.

EKS는 LT를 변경할 때 자동으로 재생성되기 때문에 ASG도 그와 같은 방식일 줄 알았지만 아니었다.

EKS는 managed AS라서 자동 재생성 되고, 그냥 ASG는 managed가 아니기 때문에 위와 같은 차이점이 있다.

---

### Jenkins

---

### GKE 버전 업그레이드

- bastion ssh 접속 (**gce**)
    - sudo -i 로 sudo 계정으로 전환
    - kubectl version —short 로 버전 확인
        - k8s에서 client는 k8s cluster와 통신하는 kubectl과 같은 것들을 의미함
    - **gke** → 클러스터 version 업그레이드 → k9s로 확인 ( k9s 확인할 때 클러스터 연결 → glcloud에서 cluster 접속하는 명령줄 복붙  → k9s )
    - 클러스터 안의 노드풀 version 업그레이드 ( 업그레이드한 클러스터 version과 같게 설정)
    - cli 명령어들
        - kubectl cordon (노드명)
        - kubectl drain —delete-local-data —grace-period-30 —ignore-daemonsets (노드명)
        - 그 후에 pod들 잘 떴는지 확인하고 노드 삭제
        
    - 콘솔로 노드풀 업그레이드를 진행해도 자동으로 노드풀이 추가로 하나 더 생김을 확인할 수 있음
    - 기존 노드풀은 cordon 상태가 되고 신규 노드풀이 생성됨을 확인. (별다른 cordon 과정 없이도)
    

---

### GCP service account

IAM 서비스 계정은 사용자가 아닌 애플리케이션에서 사용할 특수 계정

IAM 서비스 계정을 사용하여 Google Cloud service API에 인증된 호출을 수행할 수 있음

Pod는 K8s 서비스 계정을 사용하여 API 서버에 인증할 수 있음

일반적으로 GKE 노드는 Compute Engine default Service Account를 사용

- 이 권한은 GKE 노드에 필요한 것보다 많은 권한을 포함 ⇒ 최소 권한을 사용하는 역할을 사용하는 것이 권장됨

---

### 워크로드 identity

- GKE 클러스터의 K8s service 계정이 IAM 서비스 계정 역할을 수행할 수 있도록 함
    - IAM 서비스 계정을 만들고 특정 Google Cloud API에 대한 액세스 권한 부여
    - IAM 서비스 계정을 K8s 서비스 계정에 바인딩
    - 워크로드에 K8s 서비스 계정 할당
    - (서비스 계정의 과한 권한을 축소하는 역할로 쓰이는듯하다)

---

### GKE GateKeeper

 OpenSource , K8s 클러스터에서 포드 생성 및 업데이트 요청을 검증하는 허용 컨트롤러

- 관리자가 제약조건을 사용하여 정책을 정의할 수 있음
- 제약조건은 K8s에서 배포 동작을 허용하거나 거부하는 조건의 집합

---

### API vs Endpoints

endpoints : API가 서버에서 자원에 접근할 수 있도록 하는 URL

api : 두 애플리케이션이 상호작용할 수 있게 하는 프로토콜의 총집합

---

### CI CD Pipeline

파이프라인은 코드를 빌드 테스트 배포하는 과정을 거치는 개발 추진 프로세스

CI : 코드 통합, 많은 개발자들이 코드 베이스를 게속해서 통합

CD : 코드 베이스가 항상 배포 가능한 상태를 유지 ~ 모든 변경사항을 자동화된 테스트와 빌드를 거친 후 실제 사용자에게 자동으로 배포하는 것

---

### instance OS version 확인

- AMI 확인하기

또는

- 인스턴스 → 작업 → 모니터링및문제해결→인스턴스 스크린샷 가져오기

---

### CMS 연동

1. service에서 토픽을 생성
2. topic이 shard 계정의 접근을 허용
3. shared 계정의 lambda가 topic에서 호출되는 것을 허용
    1. 람다가 쓰이려면 호출이 되어야 하기 때문
4. topic을 프로토콜 lambda 방식으로 lambda가 구독함

---

# curl , ping , nslookup 비교

- curl
    - URL을 통해 데이터를 전송하고 받는 명령줄 도구
    - HTTP / HTTPS / FTP 등의 프로토콜 지원
    - Json 형식으로 응답 반환함
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0af99f2e-8fd6-4b1d-b06a-d80ab9501941/638c35f1-008d-452e-b0b2-a7820980997d/Untitled.png)
    

- ping
    - 네트워크 상태 확인하고 호스트간의 연결을 테스트하기 위해 사용되는 명령어
    - **패킷을 보내고 응답을 받음**
    - **네트워크 연결 상태 확인**
    - usecases)
        - ping [목적지] [옵션]
        - 기본적으로 4회 수행. -t 옵션을 넣어주면 반복 수행
            - ping 127.0.0.1
            - ping [google.com](http://google.com)
        
        ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0af99f2e-8fd6-4b1d-b06a-d80ab9501941/532b76d1-cb71-40f9-a14d-87a0305ffc2e/Untitled.png)
        
    
    [google.com](http://google.com) 과 [www.google.com](http://www.google.com) 의 ip 주소가 다르게 반환됨
    
    **WHY?** ⇒ DNS의 작동 방식과 관련이 있음
    
    ping [google.com](http://google.com)  ⇒ google.com 도메인의 IP 주소를 찾음 → DNS 구글 자체 IP 주소 반환
    
    ping [www.google.com](http://www.google.com) ⇒ google의 서브도메인. DNS에 따라 별도의 IP 주소를 가질 수 있기 때문에 다른 결과가 반환될 수 있음.
    
    ***** 시간을 두고 재시도 결과 같게 반환함. 구글에서 IP 여러개 써서 위와 같은 결과가 있던듯
    
     
    
    ![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/0af99f2e-8fd6-4b1d-b06a-d80ab9501941/da8a4963-2e16-4830-97f5-7fd1d5432d95/Untitled.png)
    
    ping https://www.google.com 안되는 이유 : ping은 주어진 호스트에 ICMP 패킷을 보내 응답을 받는데 사용되기 때문에 http 프로토콜을 지원안함 ( ICMP 프로토콜은 네트워크 장치 간에 제어 메시지를 교환하는 데 사용)
    
    ps) 172 대역 → 사설 IP 대역 (B class),, 250*250 개의 ip가 같은 장비에 붙을 수 있음,, 192 대역은 C class로 250개의 장비가 하나에 붙음 ,,, 10 대역  A class 대역 —> 통상적인 것이기 때문에 다른 형태를 띌 수도 있음
    
- nslookup
    - 네트워크간의 통신 여부보다는 DNS만을 찾음
    

---

**와탭**

- 와탭 k8s
- 와탭 인프라
- 와탭 apm
- 와탭 dpm

---

AWS CLI → eks 접속

bastion을 통해서 들어가는 게 맞음

prod c 에 있음

---

**alias 설정**

---

GKE 버전 지원 종료되면 언제 자동 업그레이드 될 지 모름.

---

### Alias 설정

vi .bashrc → 알리아스 넣고 저장

---

### Type : Cluster vs LB

 

---

### DB connection

pod HPA 로 인해 발생?

→ 와탭에서 이벤트 기록 확인하여 CMS 알림 온 시점에서의 문제되는 애플리케이션 확인

→ 확인 결과 cs-was 문제였음 → 컨테이너 맵 → 필터에 디플로이먼트 cs-was 걸고 시간대 설정해서 pod 수 변경 있었음을 확인 → pod가 늘어남으로 인해 DB 커넥션이 일시적 증가하여 알림 발생한 것임

---

**SNMS 에이전트 실행**

- ps aux | grep Up 를 통해 SNMS 에이전트 실행 여부 확인
- 없다면, cd /opt → cd UplusSmAgent →cd bin → ./Asmsd.sh start
- snms 들어가서 확인

---

### Endpoints vs api

api → 프로토콜간의 통신 규약

endpoints → 그 api의 주소

### DetachNetworkEndpoints

---

Kubectl 구성하기

config 파일을 통해 aws 프로젝트와 연결이 되어 있어야 함.

.kube/config 파일이 중요함

---

### cloudformation

- 기존 콘솔로 만들어진 인프라에 대해서도 scan 기능을 통해 code 처럼 관리 가능
    - 테라폼과 비슷한듯? → cloudformation도 scan 안하고 code 변경하면 오류 뜨나?

---

### stack policy

스택으로 관리 가능한 자원들에 대해 제어 가능한 범위를 설정할 수 있음

---

### CodeDeploy

---

실습 따라하며 기록 남기기.

자격 증명 정책 / 리소스 정책

리소스 정책

---

### Readiness probe - Liveness probe

Liveness ⇒ 일정 간격으로 Health check → 문제 발생 지속 ⇒ Container 재기동

Readiness ⇒ 일정 간격으로 Health check → 문제 발생 지속 ⇒ Service Endpoint에서 해당 Pod IP 제거하여 요청 안 받음

---

GCP 보안 취약점

GAC 자격 증명 사용중인데, GAC를 쓰지 말아라?

노드 서비스 계정 → 

---

### GCP 인증서

= 인증서 확인은 Security ⇒ Certificate Manager 에서 확인한다.

=  또는 LB의 하단 메뉴중 component view 를 통해 CERTIFICATES 에 들어가고 , 만료될 인증서의 PROXY 명을 TARGET PROXY 명에서 검색하면 그에 해당하는 LB를 확인할 수 있다.

GCP 인증서를 보면 in use by (proxy)로 명시되어 있는데

프록시 자원을 GCP에서 확인할 수 없었다.

GCP는 L7 LB를 만들면, proxy 기반 LB로 생성을 해주는데 , 이 proxy는 GCP managed 자원이다.

외부에서 L7 Traffic이 오면 ⇒ Prxoy 서브넷의 proxy를 타고 ⇒ FE LB ⇒ BE LB 순서로 트래픽이 흘러감.
